{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class Assignment 1\n",
    "\n",
    "This assignment will first cover regular expressions, a.k.a RegEx. After that, we will make use of them to tokenize and normalize two different types of text. You can always check chapter 2 to make sure what operators you need for a specific task.\n",
    "\n",
    "Let's start with some RegEx exercises!\n",
    "\n",
    "## 1. Regular Expressions\n",
    "\n",
    "We will use data from Twitter for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets have a date me and you for ever and ever üòçüòò #NeverAlone\r\n",
      "ni el header he hecho....\r\n",
      "va tener un hijo...confrsion es mioesta urbanisacion es enorme...\r\n",
      "deseo acabar con todo...\r\n",
      "@NEWdanicsierra em que?\r\n"
     ]
    }
   ],
   "source": [
    "twitter_file = 'twitter.txt'\n",
    "!head -5 {twitter_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Using grep, find all the tweets (lines) that start with capital letters and either have a twitter username (@username) or a hashtag at the end.\n",
    "\n",
    "**Hint**: you should get 26 lines. Use the pipe operator to count the lines: ``!grep ... | wc -l``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lets have a date me and you for ever and ever üòçüòò #NeverAlone',\n",
       " 'Me encantaria que me vieras, de la manera que yo te veo #OneDay',\n",
       " \"Going asleep with papi's music on skype, even thoe our call hang up #Priceless #DistanceDontMatter\",\n",
       " 'Que rico me duele mi cuerpo #goodexercise',\n",
       " 'La conversaci√≥n de precol mas vale que continue! @mememecaigo @PattyB_14  @salim_rosa',\n",
       " 'Ya sabes mi bro arjona me gusta üòäüòè @Abnerdm',\n",
       " 'I feel so sad üòîüòû #GodPleaseGuideMe',\n",
       " 'Movie is about to begin #SonOfGod',\n",
       " 'I hate the fact people get into my room and stuff start missing üòîüòÅüò© #WTH',\n",
       " 'FULL RIDE :o #PARY',\n",
       " 'I got 96 in my test #purocerebro',\n",
       " 'Nike is making some pretty kiks #ilovethem',\n",
       " 'DONT LIE TO ME ‚ùåüö´üòÅ #backofme',\n",
       " 'That sucks! But I like #MatthewMcConaughey',\n",
       " 'I wanna go to sleep but I gotta see #Leo win that first Oscar #Oscars',\n",
       " 'I take naps serious ..digo 15 mins. me le anto 3 horas despues-.- #always',\n",
       " \"I'm always showing off @ReguloCaro 's daughter as if she's my own haha. #cutestkidevva\",\n",
       " \"Okay the match is over, horrible refereeing that definitely benefited Real Madrid. That's soccer for you. I'll take the tie (2-2) #lideres\",\n",
       " 'I must wake up early tomorrow! #mustgetworkdone',\n",
       " 'Hoy en ingles .tidos somos carlos perez! jajaja @nataliesolange  @mememecaigo  @LoubrielClaudia',\n",
       " \"Happy B-Day to my boi @DCB4L y'all be sure to wish the homie a Happy B-Day! #TrillHomie\",\n",
       " \"Already my boi! Salute from da Dirty D! Dallas Texas fucks wit y'all FLA Mexicanos! Y'all check out my boi @CityboiChiko561 jam! Jammin@RepDeportiva 4 veces #RepGana\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex11_sol = !grep -E \"^[A-Z].*(@[a-zA-Z0-9_]+|#[A-Za-z0-9_]+)$\" {twitter_file}\n",
    "ex11_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Do the same exercise but using Python this time. \n",
    "\n",
    "**Hint**: the regular expression should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Lets have a date me and you for ever and ever üòçüòò #NeverAlone',\n",
       " 'Me encantaria que me vieras, de la manera que yo te veo #OneDay',\n",
       " \"Going asleep with papi's music on skype, even thoe our call hang up #Priceless #DistanceDontMatter\",\n",
       " 'Que rico me duele mi cuerpo #goodexercise',\n",
       " 'La conversaci√≥n de precol mas vale que continue! @mememecaigo @PattyB_14  @salim_rosa',\n",
       " 'Ya sabes mi bro arjona me gusta üòäüòè @Abnerdm',\n",
       " 'I feel so sad üòîüòû #GodPleaseGuideMe',\n",
       " 'Movie is about to begin #SonOfGod',\n",
       " 'I hate the fact people get into my room and stuff start missing üòîüòÅüò© #WTH',\n",
       " 'FULL RIDE :o #PARY',\n",
       " 'I got 96 in my test #purocerebro',\n",
       " 'Nike is making some pretty kiks #ilovethem',\n",
       " 'DONT LIE TO ME ‚ùåüö´üòÅ #backofme',\n",
       " 'That sucks! But I like #MatthewMcConaughey',\n",
       " 'I wanna go to sleep but I gotta see #Leo win that first Oscar #Oscars',\n",
       " 'I take naps serious ..digo 15 mins. me le anto 3 horas despues-.- #always',\n",
       " \"I'm always showing off @ReguloCaro 's daughter as if she's my own haha. #cutestkidevva\",\n",
       " \"Okay the match is over, horrible refereeing that definitely benefited Real Madrid. That's soccer for you. I'll take the tie (2-2) #lideres\",\n",
       " 'I must wake up early tomorrow! #mustgetworkdone',\n",
       " 'Hoy en ingles .tidos somos carlos perez! jajaja @nataliesolange  @mememecaigo  @LoubrielClaudia',\n",
       " \"Happy B-Day to my boi @DCB4L y'all be sure to wish the homie a Happy B-Day! #TrillHomie\",\n",
       " \"Already my boi! Salute from da Dirty D! Dallas Texas fucks wit y'all FLA Mexicanos! Y'all check out my boi @CityboiChiko561 jam! Jammin@RepDeportiva 4 veces #RepGana\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def read_file(file):\n",
    "    return [l.rstrip() for l in open(file)]\n",
    "\n",
    "def ex12_sol(file):\n",
    "    pattern = r'^[A-Z].*(@[a-zA-Z0-9_]+|#[A-Za-z0-9_]+)$'\n",
    "    return [l for l in read_file(file) if re.findall(pattern, l)]\n",
    "\n",
    "print(len(ex12_sol(twitter_file)))\n",
    "ex12_sol(twitter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Find all the tweets that contains emojis\n",
    "\n",
    "**Hint**: Use the following unicode block: `x1F000 - x2F000`.\n",
    "\n",
    "If we were to search using grep, we would need to keep in mind that the file is written in UTF-8, and as such we need to use the hexadecimal values of the bytes in the regex pattern (you can use this link to see the conversion: https://r12a.github.io/apps/conversion/). For our unicode block we would have the following:\n",
    "* `x1F000 -> F0 9F 80 80`\n",
    "* `x2F000 -> F0 AF 80 80`\n",
    "\n",
    "The command to grep a specific unicode in bytes would be like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äú@itseffinjas: I'm eating like I'm pregnant üò≥‚Äù Maybe u is\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"\"$'\\xF0\\x9F\\x98\\xB3'\"\" {twitter_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the search for all the emojis but this time using Python (you won't need the byte representation this time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Lets have a date me and you for ever and ever üòçüòò #NeverAlone',\n",
       " 'I want friday to be here already :) can wait I just hope it fit üòÅ',\n",
       " '\"Smile cause your smile is amazing\" üòÑ',\n",
       " 'A dormir goodnight üòò',\n",
       " 'I want to sleep more üò¥üò¥üòû',\n",
       " 'Un poco de miedo üòè',\n",
       " '\"@PattyB_14: Alguien hizo las preguntas de espa√±ol ? üò©\" ja! nop',\n",
       " '\"@aleeisaabeel: Bueno gracias al guineo de ayer no tengo falda para la escuela üòä\" yo no tengo camisa limpia... jajaja',\n",
       " '\"@vivivazq: @arianaasalgado no comprendo porque se rien deverdad üòÇüòÇüòÇ\" yo tampoco entiendo.. eres comediante en sus ojos o algo jajaj',\n",
       " 'Can i take my pillow to work #Please üòîüôè']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ex13_sol(file):\n",
    "    lines = read_file(file)\n",
    "    inblock = lambda c: int('01F000', 16) <= ord(c) <= int('02F000', 16)\n",
    "\n",
    "    return [l for l in lines if any(map(inblock, l))]\n",
    "\n",
    "res = ex13_sol(twitter_file)\n",
    "\n",
    "print(len(res))\n",
    "res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Lets have a date me and you for ever and ever üòçüòò #NeverAlone',\n",
       " 'I want friday to be here already :) can wait I just hope it fit üòÅ',\n",
       " '\"Smile cause your smile is amazing\" üòÑ',\n",
       " 'A dormir goodnight üòò',\n",
       " 'I want to sleep more üò¥üò¥üòû',\n",
       " 'Un poco de miedo üòè',\n",
       " '\"@PattyB_14: Alguien hizo las preguntas de espa√±ol ? üò©\" ja! nop',\n",
       " '\"@aleeisaabeel: Bueno gracias al guineo de ayer no tengo falda para la escuela üòä\" yo no tengo camisa limpia... jajaja',\n",
       " '\"@vivivazq: @arianaasalgado no comprendo porque se rien deverdad üòÇüòÇüòÇ\" yo tampoco entiendo.. eres comediante en sus ojos o algo jajaj',\n",
       " 'Can i take my pillow to work #Please üòîüôè']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = !grep \"[\"$'\\xF0\\x9F\\x80\\x80'\"-\"$'\\xF0\\xAF\\x80\\x80'\"]\" twitter.txt \n",
    "\n",
    "print(len(res))\n",
    "res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization can get complicated with a lot of edge cases. This time, we will just implement a toy tokenization function. Your function need to consider the following cases:\n",
    "* Split by spaces, tabs, and enters: \n",
    "    * `'  this    is a tweet   ' -> 'this', 'is', 'a', 'tweet'`\n",
    "* Split by emojis (keeping them):\n",
    "    * `haha üòÇüòÇ! -> 'haha', 'üòÇüòÇ', '!'`\n",
    "* Keep hashtags intact (i.e., do not split inside a hashtag):\n",
    "    * `#don'tSplit `\n",
    "* Separate punctuation marks from the text as individual tokens:\n",
    "    * `'hey!' -> 'hey', '!'`\n",
    "* Contractions need to be separated based on the apostrophe (without removing it):\n",
    "    * `\"that's fine\" -> 'that', \"'s\", 'fine'`\n",
    "    * `\"don't do that\" -> 'do', \"n't\", 'do', 'that'`\n",
    "\n",
    "**Hint**: Split using a cascade approach. That is, do not try to cover all the cases at once ‚Äîdivide and conquer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lets', 'have', 'a', 'date', 'me', 'and', 'you', 'for', 'ever', 'and', 'ever', 'üòç', 'üòò', '#NeverAlone']\n",
      "['ni', 'el', 'header', 'he', 'hecho', '....']\n",
      "['va', 'tener', 'un', 'hijo', '...', 'confrsion', 'es', 'mioesta', 'urbanisacion', 'es', 'enorme', '...']\n",
      "['deseo', 'acabar', 'con', 'todo', '...']\n",
      "['@NEWdanicsierra', 'em', 'que', '?']\n",
      "['no', 'hay', 'nadie', 'en', 'la', 'escuela', 'todavia', 'cual', 'es', 'proposito', '....']\n",
      "['@esthimarie', 'ustedes', 'pidieron', 'las', 'camisas', 'exactas', '?', 'pq', 'mi', 'mama', 'se', 'antojo', 'de', 'una', 'ahora', '-.-']\n",
      "['Me', 'encantaria', 'que', 'me', 'vieras', ',', 'de', 'la', 'manera', 'que', 'yo', 'te', 'veo', '#OneDay']\n",
      "['me', 'queje', 'de', 'las', 'preguntas', 'por', 'dos', 'horas', '...', 'las', 'termine', 'en', 'menos', 'de', 'media', 'hora']\n",
      "['mis', 'padres', 'discutiendo', 'quien', 'grabo', 'mejor', 'el', 'baile', '..', 'jajajaja']\n",
      "['reacci√≥na', 'encabronada', 'como', 'si', 'el', 'sue√±o', 'fuese', 'verdad', '..', 'jajajaja']\n",
      "['@quinti14', 'ouchh', '..', 'como', 'te', 'lo', 'doblaste', '?']\n",
      "['@esthimarie', 'oka']\n",
      "['dorm√≠', '15', 'horas', '...']\n",
      "['@NEWdanicsierra', 'como', 'pagamos', 'pal', 'de', '$$$', 'por', 'ma√±ana', 'yo', 'no', 'me', 'lo', 'quito', ':)']\n",
      "['this', 'can', 'be', 'happing', 'to', 'me', 'Noooooo', 'wake', 'up', 'girl', '!!!']\n",
      "['no', 'poder', 'ir', 'a', 'la', 'playa', 'porque', 'no', 'he', 'terminado', 'de', 'estudiar', 'me', 'duele', ':/']\n",
      "['Bout', 'to', 'get', 'my', 'Fat', 'ON']\n",
      "['este', 'cielito', 'lindo', '!', ':)']\n",
      "['@Liberty316', 'Ur', 'B-Day', 'was', 'yesterday', 'too', 'fam', '!?']\n",
      "['I', 'just', 'want', 'to', 'slap', 'my', 'self', 'jaja', 'stop', 'it', 'Alejandra', 'stop', 'it']\n",
      "['antes', 'de', 'comer', ':\"', 'c√°llate', 'no', 'me', 'hables', ',', 'te', 'odio\"', 'despues', 'de', 'comer', ':\"', 'c√°llate', 'no', 'me', 'hables', ',', 'estoy', 'feliz\"']\n",
      "['@PMGA_', 'jajajajaaj', 'mi', 'papa', 'hiso', 'lo', 'mismo', '!']\n",
      "['no', 'siento', 'mis', 'piernas', '-.-']\n",
      "['@alexandrarosado', 'el', 'mio', 'esta', 'tan', 'chaputeao', 'que', 'me', 'da', 'pena']\n",
      "['11:11', '.....', 'make', 'a', 'wish', '.......', 'night', 'night']\n",
      "['@mememecaigo', 'dime', 'que', 'eso', 'no', 'es', 'para', 'ma√±ana', 'y', 'que', 'es', 'para', 'el', '5', ':(']\n",
      "['I', 'want', 'friday', 'to', 'be', 'here', 'already', ':)', 'can', 'wait', 'I', 'just', 'hope', 'it', 'fit', 'üòÅ']\n",
      "['@yandiaflandia', 'vivi', 'durmio', 'como', '18', '...']\n",
      "['sigo', 'viendo', 'el', 'baile', 'de', 'nosotras', 'y', 'sigo', 'llorando']\n",
      "['Good', 'Night', 'World', 'y', \"'all\", 'be', 'blessed']\n",
      "['\"Smile', 'cause', 'your', 'smile', 'is', 'amazing\"', 'üòÑ']\n",
      "['I', 'already', 'know', 'what', 'I', 'want', 'for', 'my', 'b-day', ':)']\n",
      "['Am', 'going', 'to', 'work', 'really', 'hard', '!!!', 'No', 'matter', 'what', 'is', 'going', 'in', 'my', 'life']\n",
      "['@YiseBabee', 'i', 'wish', 'i', 'could', 'be', 'ignorant']\n",
      "['mi', 'tia', 'llorando', 'viendo', 'el', 'video', 'esto', 'no', 'pasa']\n",
      "['\"', '¬ø', 'vas', 'hacer', 'algo', 'hoy', '?', '\"', '-sii', ',', 'decir', 'que', 'voy', 'a', 'estudiar', 'y', 'terminar', 'trabajos', 'y', 'no', 'hacer', 'tres', 'carajos', '...']\n",
      "['que', 'empieze', 'ya', '!']\n",
      "['lol', 'ELLEN', 'saying', 'she', 'has', 'no', 'money', 'to', 'tip', 'the', 'pizza', 'boy', \":')\"]\n",
      "['Good', 'Afternoon', 'World']\n",
      "['@alexiasalazar8', 'meow']\n",
      "['I', 'hope', 'it', 'can', 'happen', '!!!', 'if', 'not', 'it', 'ok', 'pero', 'if', 'it', 'does', ':)', 'please', 'please', 'let', 'it', 'happen', '<3']\n",
      "['la', 'gente', 'que', 'no', 'fue', 'por', 'cobarde', 'se', 'lo', 'perdio', '!']\n",
      "['Going', 'asleep', 'with', 'papi', \"'s\", 'music', 'on', 'skype', ',', 'even', 'thoe', 'our', 'call', 'hang', 'up', '#Priceless', '#DistanceDontMatter']\n",
      "['@esangiecarajo', 'lo', 'que', 'tu', 'digas', 'jajajaja']\n",
      "['my', 'mom', ':', 'me', 'puestes', 'mandar', 'unos', 'gotero', 'que', 'se', 'llama', '\"me', 'vale', 'madre\"', 'jajajaja']\n",
      "['A', 'dormir', 'goodnight', 'üòò']\n",
      "['mil', 'cosas', 'que', 'terminar', 'y', 'yo', 'sigo', 'aqui', 'en', 'mi', 'cama', 'esperando', 'que', 'de', 'alguna', 'manera', 'se', 'hagan', '...']\n",
      "['este', 'dia', 'se', 'me', 'ha', 'hecho', 'eterno']\n",
      "['como', 'es', 'posible', 'que', 'dormi', 'tanto', 'y', 'quiero', 'seguir', 'durmiendo', '...']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def apply_pattern(tokens, cond, fun):\n",
    "    for t in tokens:\n",
    "        if cond(t):\n",
    "            for tt in fun(t):\n",
    "                if tt:\n",
    "                    yield tt\n",
    "        else:\n",
    "            yield t\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = sentence.strip().split()\n",
    "\n",
    "    # Regex to separate emojis (using complement method)\n",
    "    regex_emojis = r'[^\\w\\s0-9{}]'.format(string.punctuation)\n",
    "    tokens = apply_pattern(tokens,\n",
    "                           lambda t: re.findall('{}'.format(regex_emojis), t),\n",
    "                           lambda t: re.split('({})'.format(regex_emojis), t))\n",
    "\n",
    "    # Regex for characters that we always need to separate (e.g., for 'heey!hey' we separate by '!')\n",
    "    regex_alwayssplit = '([!?]+)'\n",
    "    tokens = apply_pattern(tokens,\n",
    "                           lambda t: re.findall('{}'.format(regex_alwayssplit), t),\n",
    "                           lambda t: re.split('{}'.format(regex_alwayssplit), t))\n",
    "\n",
    "    # Regex to separate punctuations at the end of the token\n",
    "    regex_punctuation = '[.,;:!?]+'\n",
    "    tokens = apply_pattern(tokens,\n",
    "                           lambda t: re.findall('.+{}$'.format(regex_punctuation), t),\n",
    "                           lambda t: re.split('({})'.format(regex_punctuation), t))\n",
    "\n",
    "    # Regex to separate contractions\n",
    "    regex_contraction = \"('s+|n't+|'ll+|'m+|'ve+|'d+|'re+|'all)\"\n",
    "    tokens = apply_pattern(tokens,\n",
    "                           lambda t: re.findall(\"([^#]|^)\\w+{}$\".format(regex_contraction), t, flags=re.IGNORECASE),\n",
    "                           lambda t: re.split(regex_contraction, t, flags=re.IGNORECASE))\n",
    "\n",
    "    # Regex to separate some emojis made from punctuation expressions\n",
    "    regex_specialcase = \"([:;\\-\\\"']{1,2}[dsco]{1}$|[.,:;/\\-\\\"'\\(\\)¬ø?¬°!]{2,})\"\n",
    "    tokens = apply_pattern(tokens,\n",
    "                           lambda t: re.findall(\"{}\".format(regex_specialcase), t, flags=re.IGNORECASE),\n",
    "                           lambda t: re.split(regex_specialcase, t, flags=re.IGNORECASE))\n",
    "\n",
    "    # Regex to separate words with a comma in the middle\n",
    "    regex_specialsearch = '([a-z]+[,][a-z]+)'\n",
    "    regex_specialsplit  = '(,)'\n",
    "    tokens = apply_pattern(tokens,\n",
    "                           lambda t: re.findall(\"{}\".format(regex_specialsearch), t, flags=re.IGNORECASE),\n",
    "                           lambda t: re.split(regex_specialsplit, t, flags=re.IGNORECASE))\n",
    "    return list(tokens)\n",
    "\n",
    "\n",
    "def ex2_sol(file):\n",
    "    return [tokenize(l.rstrip()) for l in open(file)]\n",
    "\n",
    "for elem in ex2_sol(twitter_file)[:50]:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization\n",
    "\n",
    "Based on the previous tokenization, we will perform a normalization step. To keep it simple, we will only handle the case in which a sequence of letters has a repeated pattern. Look at the following examples:\n",
    "* `hahahahaha -> haha`\n",
    "* `cooooool -> cool`\n",
    "* `!?!?!?!!!! -> !?!?!!`\n",
    "* `!!!!!????? -> !!??`\n",
    "\n",
    "**Hint:** Note that the reapeated patterns are reduce to 2 ocurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['haha', 'cool', '!?!?!!', '!!??']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize(token):\n",
    "    regex_three_letter = r\"(...)(\\1{2,})\"\n",
    "    regex_two_letter = r\"(..)(\\1{2,})\"\n",
    "    regex_one_letter = r\"(.)(\\1{2,})\"\n",
    "\n",
    "    regex_replace = r\"\\1\\1\"\n",
    "\n",
    "    token = re.sub(regex_three_letter, regex_replace, token)\n",
    "    token = re.sub(regex_two_letter, regex_replace, token)\n",
    "    token = re.sub(regex_one_letter, regex_replace, token)\n",
    "\n",
    "    return token\n",
    "\n",
    "def ex3_sol(tokenized_tweets):\n",
    "    return [[normalize(t) for t in tokens] for tokens in tokenized_tweets]\n",
    "\n",
    "ex3_sol([['hahahahaha', 'cooooool', '!?!?!?!!!!', '!!!!!?????']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grade_ex11():\n",
    "    solution = [60, 63, 98, 41, 85, 43, 34, 33, 72, 18, 32, 42, 28, 42, 69, 73, 86, 138, 47, 95, 87, 165]\n",
    "    result = [len(t) for t in ex11_sol]\n",
    "    return solution == result\n",
    "\n",
    "def grade_ex12():\n",
    "    solution = [60, 63, 98, 41, 85, 43, 34, 33, 72, 18, 32, 42, 28, 42, 69, 73, 86, 138, 47, 95, 87, 165]\n",
    "    result = [len(t) for t in ex12_sol(twitter_file)]\n",
    "    return solution == result\n",
    "\n",
    "def grade_ex13():\n",
    "    solution = [60, 65, 37, 20, 24, 18, 63, 117, 132, 39, 73, 43, 14, 55, 33, 18, 47, 43, 92, 54, 34, 31, 97, 55, 17, 73, 126, 45, 133, 72, 39, 41, 24, 58, 23, 25, 20, 37, 31, 33, 28, 20, 24, 82, 83, 114, 134, 72, 78, 139, 101, 94, 139, 46, 31, 16, 19, 18, 22, 35, 49, 5, 55, 34, 60, 137, 40, 54, 118, 19, 57, 43]\n",
    "    result = [len(t) for t in ex13_sol(twitter_file)]\n",
    "    if len(solution) <= len(result) <= len(solution) + 10: # margin of error\n",
    "        return all([tweet in result for tweet in solution])\n",
    "    return False\n",
    "\n",
    "def grade_ex2():\n",
    "    solution = [14, 6, 12, 5, 4, 11, 16, 14, 16, 10, 10, 8, 2, 4, 14, 11, 15, 6, 5, 8, 13, 22, 8, 5, 10, 8, 15, 16, 6, 9, 7, 7, 10, 15, 7, 9, 24, 4, 13, 3, 2, 21, 11, 17, 6, 15, 4, 19, 7, 11, 11, 6, 4, 4, 3, 2, 8, 4, 5, 17, 5, 16, 7, 6, 6, 6, 15, 11, 12, 7, 12, 7, 10, 2, 4, 10, 5, 2, 15, 5, 3, 12, 5, 18, 15, 4, 23, 8, 4, 4, 9, 11, 1, 3, 9, 5, 7, 11, 4, 12, 5, 4, 5, 5, 7, 5, 3, 20, 11, 10, 6, 12, 14, 6, 10, 7, 8, 4, 8, 9, 10, 19, 14, 7, 8, 5, 16, 15, 2, 10, 6, 16, 8, 5, 17, 23, 10, 26, 16, 31, 9, 3, 18, 23, 9, 9, 21, 12, 2, 8, 5, 2, 4, 8, 9, 6, 7, 7, 21, 3, 11, 9, 4, 10, 13, 8, 24, 5, 6, 7, 12, 8, 9, 8, 8, 8, 5, 11, 11, 17, 2, 8, 20, 8, 9, 9, 2, 5, 8, 11, 3, 13, 6, 13, 2, 16, 15, 7, 4, 8, 14, 5, 5, 4, 2, 9, 6, 25, 29, 11, 5, 18, 11, 5, 2, 9, 3, 10, 11, 19, 19, 17, 7, 17, 4, 30, 14, 26, 6, 4, 11, 12, 2, 4, 12, 12, 4, 12, 3, 10, 3, 19, 8, 8, 19, 10, 14, 25, 10, 8, 16, 12, 5, 10, 6, 5, 12, 8, 4, 3, 10, 7, 12, 22, 9, 12, 7, 5, 10, 26, 11, 6, 6, 5, 11, 4, 16, 12, 32, 11, 24, 29, 17, 12, 11, 3, 1, 2, 4, 7, 3, 7, 14, 10, 4, 11, 10, 6, 13, 9, 12, 15, 4, 3, 5, 6, 3, 12, 6, 3, 6, 8, 10, 11, 5, 4, 8, 6, 10, 4, 8, 2, 7, 3, 7, 5, 13, 7, 7, 4, 4, 4, 16, 6, 3, 4, 10, 5, 8, 7, 1, 2, 9, 3, 4, 5, 6, 8, 10, 7, 7, 5, 4, 2, 6, 21, 1, 4, 16, 4, 9, 16, 2, 3, 9, 4, 9, 10, 2, 20, 7, 6, 8, 3, 14, 4, 12, 4, 5, 12, 7, 7, 4, 10, 31, 10, 2, 6, 18, 28, 4, 14, 12, 6, 15, 18, 6, 7, 8, 9, 8, 4, 9, 32, 7, 9, 8, 5, 10, 7, 12, 12, 35, 2, 12, 2, 7, 8, 11, 5, 13, 23, 7, 13, 18, 8, 10, 13, 24, 32, 7, 15, 9, 11, 2, 4, 14, 14, 11, 3, 11, 2, 12, 6, 15, 8, 10, 10, 7, 2, 4, 4, 19, 3, 23, 17, 14, 8, 21, 10, 5, 16, 10, 12, 14, 15, 7, 13, 24, 5, 12, 36, 15, 6, 15, 19, 11, 13, 7, 11, 23, 15, 9, 10, 19, 10, 12, 29, 14, 19, 16, 22, 12, 7, 11, 26, 13, 7, 7, 9, 13, 14, 17, 8, 10, 10, 13, 21, 8, 10, 16, 17, 13, 13, 6, 10, 28, 5, 11, 10, 6, 19, 17, 22, 22, 9, 14, 9, 19, 11, 10, 15, 24, 31, 16, 6, 9, 16, 14, 10, 14, 23, 9, 17, 8, 8, 26, 13, 25, 19, 32, 8, 10, 18, 9, 13, 5, 17, 4, 18, 16, 8, 11, 9, 10, 22, 15, 13, 24, 17, 16, 18, 7, 9, 29, 16, 9, 24, 9, 11, 13, 10, 7, 4, 20, 11, 8, 27, 5, 13, 17, 12, 10, 8, 12, 4, 19, 19, 12, 10, 8, 13, 20, 12, 9, 12, 14, 10, 25, 16, 14, 7, 18, 20, 14, 22, 23, 23, 10, 12, 5, 11, 19, 14, 10, 10, 7, 23, 27, 12, 15, 20, 9, 15, 13, 16, 7, 7, 14, 18, 14, 29, 12, 13, 12, 11, 19, 8, 12, 16, 9, 7, 9, 5, 20, 10, 26, 10, 24, 7, 10, 25, 14, 10, 24, 14, 16, 29, 5, 35, 17, 12, 7, 14, 28, 7, 33, 7, 14, 18, 6, 16, 4, 9, 12, 18, 4, 11, 15, 14, 13, 19, 12, 13, 10, 14, 12, 6, 23, 22, 8, 17, 13, 14, 15, 27, 14, 10, 4, 6, 4, 5, 7, 6, 7, 10, 4, 11, 5, 8, 9, 8, 9, 6, 3, 7, 5, 8, 10, 11, 6, 2, 11, 5, 6, 7, 20, 7, 12, 19, 5, 15, 12, 6, 8, 25, 2, 13, 14, 25, 7, 12, 19, 12, 2, 10, 7, 4, 11, 9, 9, 13, 6, 15, 13, 6, 12, 10, 5, 4, 7, 8, 18, 9, 6, 7, 11, 4, 7, 9, 20, 7, 24, 9, 9, 10, 9, 30, 15, 4, 5, 5, 15, 11, 10, 24, 4, 11, 23, 16, 32, 9, 10, 13, 20, 7, 22, 21, 12, 32, 26, 24, 10, 19, 12, 17, 4, 32, 8, 12, 4, 15, 10, 23]\n",
    "    result = [len(t) for t in ex2_sol(twitter_file)]\n",
    "    total = len(solution)\n",
    "    correct = len([i for i in range(len(solution)) if (solution[i] -2) <= result[i] <= (solution[i] + 2)])\n",
    "    return (correct / total) > 0.95\n",
    "\n",
    "def grade_ex3():\n",
    "    test_cases = [['hahahahaha', 'cooooool', '!?!?!?!!!!', '!!!!!?????']]\n",
    "    res_cases = [['haha', 'cool', '!?!?!!', '!!??']]\n",
    "    return res_cases == ex3_sol(test_cases)\n",
    "\n",
    "def grader():\n",
    "    exs = [ grade_ex11, grade_ex12, grade_ex13, grade_ex2, grade_ex3]\n",
    "    return 100 * len(list(filter(lambda i:i(), exs))) / len(exs)\n",
    "\n",
    "grader()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
